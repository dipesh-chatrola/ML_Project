{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjWGgs8JQ4BSyY64KwGZ23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipesh-chatrola/ML_Project/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z-WPLSLhuRyP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive #toimport google drive data"
      ],
      "metadata": {
        "id": "ZpMlmfY2uYFe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI2N-k4M10FV",
        "outputId": "41f08d5f-9fc5-468d-832e-55df0192a55f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/\"My Drive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcGMfBHo14BN",
        "outputId": "72c5f874-645b-4162-b70c-ffeaeebee20d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/NEU Metal Surface Defects Data/train'"
      ],
      "metadata": {
        "id": "fj1M00H02C8I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras"
      ],
      "metadata": {
        "id": "WUbgbScc215S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "C5TK-LSrRPwm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory='/content/drive/MyDrive/NEU Metal Surface Defects Data/train'\n",
        "valid_directory='/content/drive/MyDrive/NEU Metal Surface Defects Data/valid'\n",
        "test_directory='/content/drive/MyDrive/NEU Metal Surface Defects Data/test'"
      ],
      "metadata": {
        "id": "g4QGD0VsRSno"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Path Directory:\",os.listdir(\"/content/drive/MyDrive/NEU Metal Surface Defects Data\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "egYdH2WUTmh0",
        "outputId": "71a43bcc-823b-42ce-dbf4-7b79d5c88e55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a0852cfc8f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Path Directory:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/NEU Metal Surface Defects Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Directory: \",os.listdir(\"/content/drive/MyDrive/NEU Metal Surface Defects Data/train\"))\n",
        "print(\"Testing Directory: \",os.listdir(\"/content/drive/MyDrive/NEU Metal Surface Defects Data/test\"))\n",
        "print(\"Validation Directory: \",os.listdir(\"/content/drive/MyDrive/NEU Metal Surface Defects Data/valid\"))"
      ],
      "metadata": {
        "id": "tFWPxaEUT5WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here I'm doing **dataset distribution** according to defects('Scratches', 'Pitted', 'Rolled', 'Patches', 'Inclusion', 'Crazing')"
      ],
      "metadata": {
        "id": "sQwYtaXcWRJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Scratches\n",
        "print(\"Training Scratches data:\",len(os.listdir(train_directory+'/'+'Scratches')))\n",
        "print(\"Testing Scratches data:\",len(os.listdir(test_directory+'/'+'Scratches')))\n",
        "print(\"Validation Scratches data:\",len(os.listdir(valid_directory+'/'+'Scratches')))\n",
        "\n",
        "#For Pitted\n",
        "print(\"Training Pitted data:\",len(os.listdir(train_directory+'/'+'Pitted')))\n",
        "print(\"Testing Pitted data:\",len(os.listdir(test_directory+'/'+'Pitted')))\n",
        "print(\"Validation Pitted data:\",len(os.listdir(valid_directory+'/'+'Pitted')))\n",
        "\n",
        "#For Rolled\n",
        "print(\"Training Rolled data:\",len(os.listdir(train_directory+'/'+'Rolled')))\n",
        "print(\"Testing Rolled data:\",len(os.listdir(test_directory+'/'+'Rolled')))\n",
        "print(\"Validation Rolled data:\",len(os.listdir(valid_directory+'/'+'Rolled')))\n",
        "\n",
        "#For Patches\n",
        "print(\"Training Patches data:\",len(os.listdir(train_directory+'/'+'Patches')))\n",
        "print(\"Testing Patches data:\",len(os.listdir(test_directory+'/'+'Patches')))\n",
        "print(\"Validation Patches data:\",len(os.listdir(valid_directory+'/'+'Patches')))\n",
        "\n",
        "#For Inclusion\n",
        "print(\"Training Inclusion data:\",len(os.listdir(train_directory+'/'+'Inclusion')))\n",
        "print(\"Testing Inclusion data:\",len(os.listdir(test_directory+'/'+'Inclusion')))\n",
        "print(\"Validation Inclusion data:\",len(os.listdir(valid_directory+'/'+'Inclusion')))\n",
        "\n",
        "#For Crazing\n",
        "print(\"Training Crazing data:\",len(os.listdir(train_directory+'/'+'Crazing')))\n",
        "print(\"Testing Crazing data:\",len(os.listdir(test_directory+'/'+'Crazing')))\n",
        "print(\"Validation Crazing data:\",len(os.listdir(valid_directory+'/'+'Crazing')))"
      ],
      "metadata": {
        "id": "2TABKbKmUq8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Pre-processing of DATA**"
      ],
      "metadata": {
        "id": "FKJ0-fLfcj1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "RdBA8T96U9KD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 10 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_directory,\n",
        "        target_size=(200, 200),\n",
        "        batch_size=10,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Flow validation images in batches of 10 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        valid_directory,\n",
        "        target_size=(200, 200),\n",
        "        batch_size=10,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#REFERENCE\n",
        "#https://faroit.com/keras-docs/1.2.2/preprocessing/image/\n",
        "#https://www.linkedin.com/pulse/keras-image-preprocessing-scaling-pixels-training-adwin-jahn/\n",
        "#https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/\n",
        "#https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUGIZEWqW-W1",
        "outputId": "b130bc05-eb04-4428-9b7f-9188fbf5344f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1656 images belonging to 6 classes.\n",
            "Found 72 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy') > 0.98 ):\n",
        "            print(\"\\nReached 98% accuracy so cancelling training!\")\n",
        "            self.model.stop_training = True "
      ],
      "metadata": {
        "id": "6s35S5H5Hs3Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=(200, 200, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ywyeBwITdi",
        "outputId": "e62b07c6-0db4-4b7e-96c7-bf2e366cf7d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 199, 199, 32)      416       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 99, 99, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 98, 98, 64)        8256      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 49, 49, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 48, 48, 128)       32896     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 24, 24, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 73728)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               18874624  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,917,734\n",
            "Trainable params: 18,917,734\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "print('Compiled!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xuj58AaIV99",
        "outputId": "d1a68fb8-7ae7-4206-c535-8b82161a8ac4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiled!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = myCallback()\n",
        "history = model.fit(train_generator,\n",
        "        batch_size = 32,\n",
        "        epochs=20,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[callbacks],\n",
        "        verbose=1, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGYfoBh8Iafh",
        "outputId": "d16168a1-c02d-470b-d119-4961e5f2b3f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "166/166 [==============================] - 375s 2s/step - loss: 1.9009 - accuracy: 0.3086 - val_loss: 1.3552 - val_accuracy: 0.4583\n",
            "Epoch 2/20\n",
            "166/166 [==============================] - 126s 756ms/step - loss: 1.2622 - accuracy: 0.5260 - val_loss: 1.3932 - val_accuracy: 0.5278\n",
            "Epoch 3/20\n",
            "166/166 [==============================] - 128s 768ms/step - loss: 0.9762 - accuracy: 0.6576 - val_loss: 0.6060 - val_accuracy: 0.7083\n",
            "Epoch 4/20\n",
            "166/166 [==============================] - 128s 768ms/step - loss: 0.7654 - accuracy: 0.7434 - val_loss: 0.2706 - val_accuracy: 0.9306\n",
            "Epoch 5/20\n",
            "166/166 [==============================] - 126s 757ms/step - loss: 0.6567 - accuracy: 0.7874 - val_loss: 0.1747 - val_accuracy: 0.9583\n",
            "Epoch 6/20\n",
            "166/166 [==============================] - 127s 762ms/step - loss: 0.5436 - accuracy: 0.8400 - val_loss: 0.3073 - val_accuracy: 0.8611\n",
            "Epoch 7/20\n",
            "166/166 [==============================] - 127s 766ms/step - loss: 0.4686 - accuracy: 0.8521 - val_loss: 0.0969 - val_accuracy: 0.9861\n",
            "Epoch 8/20\n",
            "166/166 [==============================] - 127s 764ms/step - loss: 0.4399 - accuracy: 0.8756 - val_loss: 0.1974 - val_accuracy: 0.9583\n",
            "Epoch 9/20\n",
            "166/166 [==============================] - 124s 747ms/step - loss: 0.4122 - accuracy: 0.8708 - val_loss: 0.0588 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "166/166 [==============================] - 124s 749ms/step - loss: 0.4291 - accuracy: 0.8841 - val_loss: 0.2220 - val_accuracy: 0.9583\n",
            "Epoch 11/20\n",
            "166/166 [==============================] - 124s 744ms/step - loss: 0.3755 - accuracy: 0.8986 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "166/166 [==============================] - 124s 747ms/step - loss: 0.3461 - accuracy: 0.9028 - val_loss: 0.1624 - val_accuracy: 0.9722\n",
            "Epoch 13/20\n",
            "166/166 [==============================] - 124s 747ms/step - loss: 0.3953 - accuracy: 0.9058 - val_loss: 0.3376 - val_accuracy: 0.9444\n",
            "Epoch 14/20\n",
            "166/166 [==============================] - 124s 749ms/step - loss: 0.2914 - accuracy: 0.9173 - val_loss: 0.0906 - val_accuracy: 0.9861\n",
            "Epoch 15/20\n",
            "166/166 [==============================] - 124s 749ms/step - loss: 0.3184 - accuracy: 0.9124 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "166/166 [==============================] - 124s 746ms/step - loss: 0.2797 - accuracy: 0.9275 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "166/166 [==============================] - 124s 749ms/step - loss: 0.3319 - accuracy: 0.9167 - val_loss: 0.0835 - val_accuracy: 0.9861\n",
            "Epoch 18/20\n",
            "166/166 [==============================] - 124s 748ms/step - loss: 0.3218 - accuracy: 0.9173 - val_loss: 0.1644 - val_accuracy: 0.9444\n",
            "Epoch 19/20\n",
            "166/166 [==============================] - 124s 747ms/step - loss: 0.2749 - accuracy: 0.9275 - val_loss: 0.3348 - val_accuracy: 0.9444\n",
            "Epoch 20/20\n",
            "166/166 [==============================] - 124s 748ms/step - loss: 0.4124 - accuracy: 0.9185 - val_loss: 0.0455 - val_accuracy: 0.9861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "fig=plt.figure(1)  \n",
        "# summarize history for accuracy  \n",
        "plt.subplot(211)  \n",
        "plt.plot(history.history['accuracy'])  \n",
        "plt.plot(history.history['val_accuracy'])  \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'test'], loc='upper left')  \n",
        "fig.tight_layout()\n",
        "\n",
        " # summarize history for loss  \n",
        "\n",
        "plt.subplot(212)  \n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'])  \n",
        "plt.plot(history.history['val_loss'])  \n",
        "plt.title('model loss')  \n",
        "plt.ylabel('loss')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig.tight_layout()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "609wO8AJIdKb",
        "outputId": "0c74f0f3-3461-486b-d6f5-b917b5223cc5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-55dee0d4be46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAACGCAYAAADQHI0rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAALqUlEQVR4nO3df4gc933G8fcTObKpkyZKdQWjX5apUltNSuwsqkugSUksK/5DCqRtZDCRg5sDN0ohKQWXQF1kAvlBGwiota+tSFKo5cR/lCt1ECa2MYQo0Qo7jqWi5Kym1l0DViLH/yiRK/npHzPurS4n7dze3s7pvs8Llpv5znyHz33Z2+fmx87INhERUa43tF1ARES0K0EQEVG4BEFEROESBBERhUsQREQULkEQEVG4vkEg6YCklyQ9f4nlkvRlSVOSnpN0S8+yPZJ+VL/2DLPwiIgYjiZ7BF8Bdlxm+QeBLfVrHPgHAElvA+4Hfg/YBtwvac1iio2IiOHrGwS2nwbOXGaVXcDXXDkMvFXSdcDtwOO2z9h+GXicywdKRES0YBjnCNYBp3rmp+u2S7VHRMQyclXbBQBIGqc6rMS111777htvvLHliiIirixHjx79qe2xQfoOIwhmgA098+vrthngfXPan5pvA7YngAmATqfjbrc7hLIiIsoh6b8H7TuMQ0OTwEfrq4duBV6x/RPgELBd0pr6JPH2ui0iIpaRvnsEkh6m+s9+raRpqiuB3ghg+0HgMeAOYAo4C3ysXnZG0gPAkXpT+2xf7qRzRES0oG8Q2L6zz3IDn7jEsgPAgcFKi4iIUcg3iyMiCpcgiIgoXIIgIqJwCYKIiMIlCCIiCpcgiIgoXIIgIqJwCYKIiMIlCCIiCpcgiIgoXIIgIqJwCYKIiMIlCCIiCpcgiIgoXIIgIqJwCYKIiMI1CgJJOySdkDQl6b55ln9J0rP164eSft6z7ELPsslhFh8REYvX5FGVq4D9wG3ANHBE0qTt46+vY/tTPet/Eri5ZxO/sP2u4ZUcERHD1GSPYBswZfuk7VeBg8Cuy6x/J/DwMIqLiIil1yQI1gGneuan67ZfIWkTsBl4oqf5GkldSYclfWjgSiMiYkn0PTS0QLuBR21f6GnbZHtG0g3AE5J+YPuF3k6SxoFxgI0bNw65pIiIuJwmewQzwIae+fV123x2M+ewkO2Z+udJ4CkuPn/w+joTtju2O2NjYw1KioiIYWkSBEeALZI2S1pN9WH/K1f/SLoRWAN8p6dtjaSr6+m1wHuA43P7RkREe/oeGrJ9XtJe4BCwCjhg+5ikfUDX9uuhsBs4aNs93W8CHpL0GlXofK73aqOIiGifLv7cbl+n03G32227jIiIK4qko7Y7g/TNN4sjIgqXIIiIKFyCICKicAmCiIjCJQgiIgqXIIiIKFyCICKicAmCiIjCJQgiIgqXIIiIKFyCICKicAmCiIjCJQgiIgqXIIiIKFyCICKicAmCiIjCNQoCSTsknZA0Jem+eZbfLem0pGfr15/2LNsj6Uf1a88wi4+IiMXr+6hKSauA/cBtwDRwRNLkPI+cfMT23jl93wbcD3QAA0frvi8PpfqIiFi0JnsE24Ap2ydtvwocBHY13P7twOO2z9Qf/o8DOwYrNSIilkKTIFgHnOqZn67b5vqwpOckPSppw0L6ShqX1JXUPX36dMPSIyJiGIZ1svjfgett/y7Vf/1fXUhn2xO2O7Y7Y2NjQyopIiKaaBIEM8CGnvn1ddv/s/0z2+fq2X8C3t20b0REtKtJEBwBtkjaLGk1sBuY7F1B0nU9szuB/6ynDwHbJa2RtAbYXrdFRMQy0feqIdvnJe2l+gBfBRywfUzSPqBrexL4c0k7gfPAGeDuuu8ZSQ9QhQnAPttnluD3iIiIAcl22zVcpNPpuNvttl1GRMQVRdJR251B+uabxRERhUsQREQULkEQEVG4BEFEROESBBERhUsQREQULkEQEVG4BEFEROESBBERhUsQREQULkEQEVG4BEFEROESBBERhUsQREQULkEQEVG4RkEgaYekE5KmJN03z/JPSzpeP7z+W5I29Sy7IOnZ+jU5t29ERLSr7xPKJK0C9gO3AdPAEUmTto/3rPYM0LF9VtK9wBeAj9TLfmH7XUOuOyIihqTJHsE2YMr2SduvAgeBXb0r2H7S9tl69jDVQ+ojIuIK0CQI1gGneuan67ZLuQf4Zs/8NZK6kg5L+tAANUZExBLqe2hoISTdBXSA9/Y0b7I9I+kG4AlJP7D9wpx+48A4wMaNG4dZUkRE9NFkj2AG2NAzv75uu4ikDwCfAXbaPvd6u+2Z+udJ4Cng5rl9bU/Y7tjujI2NLegXiIiIxWkSBEeALZI2S1oN7AYuuvpH0s3AQ1Qh8FJP+xpJV9fTa4H3AL0nmSMiomV9Dw3ZPi9pL3AIWAUcsH1M0j6ga3sS+CLwJuAbkgBetL0TuAl4SNJrVKHzuTlXG0VERMtku+0aLtLpdNztdtsuIyLiiiLpqO3OIH3zzeKIiMIlCCIiCpcgiIgoXIIgIqJwCYKIiMIlCCIiCpcgiIgoXIIgIqJwCYKIiMIlCCIiCpcgiIgoXIIgIqJwCYKIiMIlCCIiCpcgiIgoXIIgIqJwjYJA0g5JJyRNSbpvnuVXS3qkXv5dSdf3LPuruv2EpNuHV3pERAxD3yCQtArYD3wQ2ArcKWnrnNXuAV62/VvAl4DP1323Uj3j+HeAHcDf19uLiIhloskewTZgyvZJ268CB4Fdc9bZBXy1nn4UeL+qhxfvAg7aPmf7v4CpensREbFMNAmCdcCpnvnpum3edWyfB14BfqNh34iIaNFVbRcAIGkcGK9nz0l6vs16lpG1wE/bLmKZyFjMyljMyljM+u1BOzYJghlgQ8/8+rptvnWmJV0FvAX4WcO+2J4AJgAkdW13mv4CK1nGYlbGYlbGYlbGYpak7qB9mxwaOgJskbRZ0mqqk7+Tc9aZBPbU038EPGHbdfvu+qqizcAW4HuDFhsREcPXd4/A9nlJe4FDwCrggO1jkvYBXduTwD8D/yJpCjhDFRbU630dOA6cBz5h+8IS/S4RETGARucIbD8GPDan7a97pn8J/PEl+n4W+OwCappYwLorXcZiVsZiVsZiVsZi1sBjoeoITkRElCq3mIiIKFxrQbCY21asNA3G4tOSjkt6TtK3JG1qo85R6DcWPet9WJIlrdgrRpqMhaQ/qd8bxyT966hrHJUGfyMbJT0p6Zn67+SONupcapIOSHrpUpfYq/Llepyek3RLow3bHvmL6qTzC8ANwGrg+8DWOev8GfBgPb0beKSNWpfJWPwh8Gv19L0lj0W93puBp4HDQKftult8X2wBngHW1PO/2XbdLY7FBHBvPb0V+HHbdS/RWPwBcAvw/CWW3wF8ExBwK/DdJttta49gMbetWGn6joXtJ22frWcPU30fYyVq8r4AeIDqfla/HGVxI9ZkLD4O7Lf9MoDtl0Zc46g0GQsDv15PvwX4nxHWNzK2n6a6MvNSdgFfc+Uw8FZJ1/XbbltBsJjbVqw0C70Nxz1Uib8S9R2Leld3g+3/GGVhLWjyvng78HZJ35Z0WNKOkVU3Wk3G4m+AuyRNU13h+MnRlLbsDHRbn2Vxi4loRtJdQAd4b9u1tEHSG4C/A+5uuZTl4iqqw0Pvo9pLfFrSO23/vNWq2nEn8BXbfyvp96m+1/QO26+1XdiVoK09goXctoI5t61YaRrdhkPSB4DPADttnxtRbaPWbyzeDLwDeErSj6mOgU6u0BPGTd4X08Ck7f91dXffH1IFw0rTZCzuAb4OYPs7wDVU9yEqTaPPk7naCoLF3LZipek7FpJuBh6iCoGVehwY+oyF7Vdsr7V9ve3rqc6X7LQ98D1WlrEmfyP/RrU3gKS1VIeKTo6yyBFpMhYvAu8HkHQTVRCcHmmVy8Mk8NH66qFbgVds/6Rfp1YODXkRt61YaRqOxReBNwHfqM+Xv2h7Z2tFL5GGY1GEhmNxCNgu6ThwAfhL2ytur7nhWPwF8I+SPkV14vjulfiPo6SHqcJ/bX0+5H7gjQC2H6Q6P3IH1bNfzgIfa7TdFThWERGxAPlmcURE4RIEERGFSxBERBQuQRARUbgEQURE4RIEERGFSxBERBQuQRARUbj/AwbyyISJLV2dAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**VGG 16**\n",
        "\n"
      ],
      "metadata": {
        "id": "aJa--npv0Lw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout"
      ],
      "metadata": {
        "id": "gXGgtDCCTkYF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=True, weights='imagenet', input_tensor=None,\n",
        "    input_shape=None, pooling=None, classes=1000,\n",
        "    classifier_activation='softmax'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smdEdMc8rbIm",
        "outputId": "f5c8447c-0806-4cb6-9ed9-cde4f5e7b99c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7fe44cc01710>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the libraries\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from glob import glob\n",
        "\n",
        "#from keras.preprocessing import image\n",
        "\n",
        "folders = glob(train_directory + '/*')\n",
        "num_classes = len(folders)\n",
        "\n",
        "IMAGE_SIZE = [64, 64]  # we will keep the image size as (64,64). You can increase the size for better results. \n",
        "\n",
        "# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n",
        "vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n",
        "\n",
        "# this will exclude the initial layers from training phase as there are already been trained.\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(vgg.output)\n",
        "#x = Dense(128, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\n",
        "x = Dense(num_classes, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n",
        "\n",
        "model = Model(inputs = vgg.input, outputs = x)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Iy9rO6gA-o6q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHFoutoAy-KW",
        "outputId": "51bab7e7-19cf-4cf1-b3eb-75311f2eb6a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "\n",
        "# include top should be False to remove the softmax layer\n",
        "pretrained_model = VGG16(include_top=False, weights='imagenet')\n",
        "pretrained_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojd5lTCoq9P-",
        "outputId": "afaa4d53-0696-473c-dbb2-bce83862d54f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None, None, 3)]   0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input,decode_predictions"
      ],
      "metadata": {
        "id": "B1Nqc3SYyB31"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "IMAGE_SIZE = [200, 200]\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "                                    rescale=1./255,   # all pixel values will be between 0 an 1\n",
        "                                    shear_range=0.2, \n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    preprocessing_function=preprocess_input)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
        "\n",
        "training_generator = training_datagen.flow_from_directory(train_directory, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')\n",
        "validation_generator = validation_datagen.flow_from_directory(valid_directory, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stcRZCpBrA8i",
        "outputId": "e758f6e4-0512-4449-a812-b37f9b17f633"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1656 images belonging to 6 classes.\n",
            "Found 72 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The labels are stored in class_indices in dictionary form. \n",
        "# checking the labels\n",
        "training_generator.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBDvvLNO2y6E",
        "outputId": "19f8d50e-38ec-4687-ff30-bd64a892b7db"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Crazing': 0,\n",
              " 'Inclusion': 1,\n",
              " 'Patches': 2,\n",
              " 'Pitted': 3,\n",
              " 'Rolled': 4,\n",
              " 'Scratches': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wbDJVI_2972S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))"
      ],
      "metadata": {
        "id": "3nLwguhu91PC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.utils\n",
        "from keras import utils as np_utils\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "training_images = 1656\n",
        "validation_images = 72\n",
        "\n",
        "history = model.fit(training_generator,\n",
        "                   steps_per_epoch = 52,  # this should be equal to total number of images in (training set/batch_size) so in my case 1656/32 = 52 around . \n",
        "                   epochs = 20,  # change this for better results\n",
        "                   validation_data = validation_generator,\n",
        "                   validation_steps = 72)  # this should be equal to total number of images in validation set."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jpqun8I286c",
        "outputId": "197ce195-31ad-4a8e-9a4f-c0779628c840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 3/52 [>.............................] - ETA: 42:36 - loss: 1.9132 - accuracy: 0.2478"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the libraries\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense\n",
        "from glob import glob\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "IMAGE_SIZE = [200, 200]  # we will keep the image size as (64,64). You can increase the size for better results. \n",
        "\n",
        "folders = glob(train_directory + '/*')\n",
        "num_classes = len(folders)\n",
        "\n",
        "# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n",
        "vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n",
        "\n",
        "# this will exclude the initial layers from training phase as there are already been trained.\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(vgg.output)\n",
        "#x = Dense(128, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\n",
        "x = Dense(num_classes, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n",
        "\n",
        "model = Model(inputs = vgg.input, outputs = x)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "42Ildi6TvIcN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dmbgm-Z10fxE"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "djZXZXzH1wPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}